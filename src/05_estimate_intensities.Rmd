---
title: "Subnetwork -> Growth Rates"
output: html_notebook
---

```{r}
require(glmnet)
require(caret)
require(dplyr)
require(e1071)

```
#### read in the data
#### the growth has been interpolated to the Klaeger 2017 (Kuster lab) concentrations to go directly from kinase inhibition -> growth rates 

```{r}
dat <- read.csv('~/Github/subnetGRcurves/data/interpolated_conc_df.csv')
dat <- data.frame(dat)
dat$Cellline <- as.factor(dat$Cellline)
head(dat)
```

#### extract the resposne variable (y) and the predictor variables (X)
```{r}

y <- dat$Growth
X <- dat[c(2,5:dim(dat)[2])]
```
#### convert the cell lines to be one-hot/indicator encoded as opposed to a factor
```{r}

X <- as.matrix(model.matrix(~., X))
```
#### growth (y) values of 1 = no drug effect
#### growth (y) values of 0 = stop all cell growth (no death)
#### growth (y) values of -1 = complete cell death
#### these are relative values, e.g. Drug/DMSO 
#### note this is surprisingly uniform, aside from the tails
```{r}

hist(y)


```


#### this creates a downweighted weights variable to downweight particularly high growth responses
```{r}

weights <- (1 + y < .995)/2
```

#### we want to convert targ to a normalish data (we can do this via inverse cdf)
#### slight problem with things equal to one and zero, however, as these are -inf and inf in qnorm
```{r}

targ <- qnorm((y+1)/2)
```
#### threshold the +/- inf norm values to +/-0 8
```{r}

targ[targ > 8] <- 8
targ[targ < -8] <- -8
```

#### this serves as a normalized target variable. it is still a bit skewn to the right, which means many more no effect and slow growth drugs than cell death drugs

```{r}

hist(targ)


```

#### train/test split - 90/10
```{r}

sample_size <- floor(.9 * nrow(X))

set.seed(1920)
train_idx <- sample(seq_len(nrow(X)), size = sample_size)
```

#### create an X_train and X_test
#### create a target response train/test split in lieu of unnormalized y
#### subset the proper weights

```{r}

X_train <- X[train_idx, ]
X_test <- X[-train_idx, ]

y_train <- y[train_idx]
y_test <- y[-train_idx]


targ_train <- targ[train_idx]
targ_test <- targ[-train_idx]


weights_train <- weights[train_idx]
```


#### set the lambdas values for elasticnet
```{r}

lambdas <- 10^seq(-6, -1, by=.1)

```

#### this is just some parallelization libraries to speedup stuff
```{r}

require(doMC)
nfolds <- 7
registerDoMC(cores=nfolds)
```


#### weighted glmnet regression for alpha = 0.3, 0.5, 0.7
```{r}

mod3 <- cv.glmnet(x=X_train, y=targ_train, weights=weights_train, lambda = lambdas, nfolds = nfolds, parallel = TRUE, alpha=0.3)
mod5 <- cv.glmnet(x=X_train, y=targ_train, weights=weights_train, lambda = lambdas, nfolds = nfolds, parallel = TRUE, alpha=0.5)
mod7 <- cv.glmnet(x=X_train, y=targ_train, weights=weights_train, lambda = lambdas, nfolds = nfolds, parallel = TRUE, alpha=0.7)

```

#### here we plot the lambdas vs. MSE to see where we fit the models well
```{r}

plot(mod3)
plot(mod5)
plot(mod7)
```



#### LOTS OF ISSUES WITH GAM
#### Probably need to fit the GAM in Python as R's is bad at high dim inputs

```{r}
#preds_gam <- predict.gam(gmod, newdata=X_test_gam)
mse <- mean((targ_train - preds_gam)^2)
print(mse)
print(cor(preds_gam, targ_train))
preds_gam <- predict(gmod, as.data.frame(X_train))
```

#### robust elasticnet with l1 error instead of OLS error -- still being debugged
```{r}
mod_robust <- cv.glmnet(x=X_train, y=targ_train, alpha=0.5, type.measure='mae', lambda = lambdas, parallel = TRUE)
preds_robust <- predict(mod_robust, s="lambda.1se", newx = X_test)
print(cor(preds_robust, targ_test))
```

#### create an unweighted model
```{r}

mod_unweight <- glmnet(x=X_train, y=targ_train, alpha=0.5)
preds_unweight <- predict(mod_unweight, s=.01, newx = X_test)
print(cor(preds_unweight, targ_test))
```


#### examine the MSE and R2 values of the weighted elasticnet models
```{r}
preds3 <- predict(mod3, s="lambda.1se", newx = X_test)
mse3 <- mean((targ_test - preds3)^2)
preds5 <- predict(mod5, s="lambda.1se", newx = X_test)
mse5 <- mean((targ_test - preds5)^2)
preds7 <- predict(mod7, s="lambda.1se", newx = X_test)
mse7 <- mean((targ_test - preds7)^2)

print('Alpha = 0.3 ~ ')
print(mse3[1])
print(cor(preds3, targ_test))

print('Alpha = 0.5 ~ ')
print(mse5[1])
print(cor(preds5, targ_test))

print('Alpha = 0.7 ~ ')
print(mse7[1])
print(cor(preds7, targ_test))

```

#### the output will look pretty similar for many other elasticnet models, although we can certainly increase sparsity
```{r}
mod_coeffs <- coef(mod_unweight, s=0.01)
mod_coeffs
```


#### attempt to use Naive Bayes to predict high-value growth rates
#### create a two models, one with just cell line info, the other with kinases as well
```{r}
#install.packages('e1071') # ML package


is1 <- (targ_train > .995 )*1
nb <- naiveBayes(x=X_train[, grep("Celll", colnames(X_train))], y = is1) # only using cell line info

nb_full <- naiveBayes(x=dat[train_idx, c(2,5:dim(dat)[2])], y = as.factor(is1))
nb_preds <- predict(nb_full, newdata=dat[train_idx, c(2,5:dim(dat)[2])], type = "class")
```


#### use a logistic regression elasticnet to try to predict high-value as well
```{r}

bin_glmnet <- glmnet(x=X_train, y = is1, alpha = 0.5, family = "binomial")
bin_glmnet_preds <- predict(bin_glmnet, newx=X_train, type = "class", s=.01)

```

##### examine some precision recall curves -- we would mostly just like high precision, which the logistic glmnet does to some extent, although there might be room for improvement

```{r}

#install.packages('ROCR')
library(ROCR)
pred <- prediction(as.numeric(bin_glmnet_preds), is1)
plot(performance(pred, "prec", "rec"))

pred <- prediction(as.numeric(nb_preds), is1)
plot(performance(pred, "prec", "rec"))
```

#### some of the coefficients for variables vs the L1 penalty 
#### this mostly just shows the model is fitting something, if not necessarily well
```{r}

plot(bin_glmnet)
```

### extract the subnetwork clusters for group models
```{r}


louv <- as.data.frame(read.csv("~/Github/KIN_ClusteringWithAnnotations/results/weighted/louvain_small_clusters.txt", sep = "\t"))
louv
```

#### we create some groups for group elasticnet based off of the subnetworks 

```{r}

grps <- (colnames(X) %in% louv$names)*1
for (idx in which(grps == 1)){
  grps[idx] <- subset(louv, names %in% colnames(X)[idx])$cluster
}
```

#### group id = 0 is unpenalized from group lasso, so here we don't penalize the cell line indicator variables
```{r}
grps
```

#### oem is just a big data group elasticnet package
```{r}

#install.packages('oem')
library(oem)

# needs a specific type of input
X_train_big <- as.big.matrix(X_train)

# fit the model
grp_mod <- big.oem(x = X_train_big, y = targ_train,family = "gaussian", penalty = "grp.lasso.net", alpha = .5, groups = grps, intercept = FALSE)
```

#### 1) compare the r2 and MSE values for the group elasticnet (unweighted)
#### 2) compare the r2 and MSE values for the unweighted elasticnet
#### 3) compare the r2 and MSE values for the weighted, alpha = 0.5 elasticnet -- much worse
```{r}

preds_oem <- predict(grp_mod, X_test, s=.001)
print(cor(preds_oem, targ_test))
print(mean((preds_oem-targ_test)^2))


preds_unweight <- predict(mod_unweight, s=.001, newx = X_test)
print(cor(preds_unweight, targ_test))
print(mean((preds_unweight-targ_test)^2))

print(cor(preds5, targ_test))
print(mean((preds5-targ_test)^2))
```
### qq norm plots to see overfit/underfit areas
#### ideally, we want an almost perfect fit between the circles and the line
#### we can see we underfit the top (no effect) drugs slightly, but we severely underfit the cell death drugs most of the time

### order:
#### 1) unweighted alpha = 0.5 elasticnet
#### 2) weighted alpha = 0.5
#### 3) attempted robust l1 error (not OLS) elasticnet -- not super working
#### 4) group elasticnet -- this tells us important subclusters -- not very different than original unweighted elasticnet, which tells us a few subclusters are highly predictive
```{r}


qqnorm(preds_unweight-targ_test, pch=1, frame=FALSE)
qqline(preds_unweight-targ_test, col="tomato", lwd=2)

qqnorm(preds5-targ_test, pch=1, frame=FALSE)
qqline(preds5-targ_test, col="tomato", lwd=2)

qqnorm(preds_robust-targ_test, pch=1, frame=FALSE)
qqline(preds_robust-targ_test, col="tomato", lwd=2)

qqnorm(preds_oem-targ_test, pch=1, frame=FALSE)
qqline(preds_oem-targ_test, col="tomato", lwd=2)
```




Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

